---
title: "Model Performance - linear"
author:
  - name: Hakan Egeli
  - name: Soumyendu Sarkar
date: 'April 24, 2018'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=FALSE, include = FALSE}

# load the project. This will autoload the data and also create training and test datasets
library('ProjectTemplate')
load.project()

```

```{r, echo=FALSE, include=FALSE}

library(caret)

require(gridExtra)

```

# Explortory Data Analysis.

## Sales Attributes and Possible Correlations

### Correlation of Sales Summaries and Credit Limit

Since our Naive model was based on **Sales Last Year**, we wanted to see how all the sales summary variables correlated to the **Credit Limit**

```{r, echo=FALSE}
g1 <- ggplot() + 
  geom_point(aes(x = dataset$SalesLastYr, y = dataset$CreditLimit), color = 'blue') +
  xlab('Last Year\'s Sales') +
  ylab('Credit Limit') +
  scale_x_continuous(breaks=c(100000, 200000, 300000, 400000, 500000, 600000), labels=c("100K", "200K", "300K", "400K", "500K", "600K")) +
  scale_y_continuous(breaks=c(10000, 20000, 30000, 40000, 50000), labels=c("10K", "20K", "30K", "40K", "50K"))

g2 <- ggplot() + 
  geom_point(aes(x = dataset$SalesCurrentYr, y = dataset$CreditLimit), color = 'blue') +
  xlab('Current Year\'s Sales') + 
  ylab('Credit Limit') +
  scale_x_continuous(breaks=c(20000, 40000, 60000, 80000, 100000, 120000), labels=c("20K", "40K", "60K", "80K", "100K", "120K")) +
  scale_y_continuous(breaks=c(10000, 20000, 30000, 40000, 50000), labels=c("10K", "20K", "30K", "40K", "50K"))

g3 <- ggplot() + 
  geom_point(aes(x = dataset$TotalSalesLast12Mo, y = dataset$CreditLimit), color = 'blue') +
  xlab('Last 12 Mo\'s Sales') + 
  ylab('Credit Limit') +
  scale_x_continuous(breaks=c(100000, 200000, 300000, 400000, 500000, 600000), labels=c("100K", "200K", "300K", "400K", "500K", "600K")) +
  scale_y_continuous(breaks=c(10000, 20000, 30000, 40000, 50000), labels=c("10K", "20K", "30K", "40K", "50K"))

grid.arrange(g1, g2, g3, ncol=3)

```

Unfortunately, we did not see a linear correlation between any of the Sales summary variables and the Credit Limit.

### Correlation of 12 Months Sales and Major Product Category Sales

Since the major product category sales for the last 12 months make up majority of the **Last 12 Mo Sales Total** we wanted to examine which product(s) had the most significant correlation.

```{r, echo=FALSE, warning=FALSE}
g1 <- ggplot() + 
  geom_point(aes(x = dataset$PlainBandSalesLast12Mo, y = dataset$TotalSalesLast12Mo), color = 'blue') +
  xlab('Last 12 Mo\'s Plains $') +
  ylab('Last 12 Mo\'s $') +
  scale_x_continuous(limits=c(0, 100000), breaks=c(10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000), labels=c("10K", "20K", "30K", "40K", "50K", "60K", "70K", "80K", "90K", "100K")) +
  scale_y_continuous(breaks=c(100000, 200000, 300000, 400000), labels=c("100K", "200K", "300K", "400K"))

g2 <- ggplot() + 
  geom_point(aes(x = dataset$DesignBandSalesLast12Mo, y = dataset$TotalSalesLast12Mo), color = 'blue') +
  xlab('Last 12 Mo\'s Designs $') + 
  ylab('Last 12 Mo\'s $') +
  scale_x_continuous(limits=c(0, 100000), breaks=c(10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000), labels=c("10K", "20K", "30K", "40K", "50K", "60K", "70K", "80K", "90K", "100K")) +
  scale_y_continuous(breaks=c(100000, 200000, 300000, 400000), labels=c("100K", "200K", "300K", "400K"))

g3 <- ggplot() + 
  geom_point(aes(x = dataset$DiamondBandSalesLast12Mo, y = dataset$TotalSalesLast12Mo), color = 'blue') +
  xlab('Last 12 Mo\'s Diamonds $') + 
  ylab('Last 12 Mo\'s $') +
  scale_x_continuous(breaks=c(10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000), labels=c("10K", "20K", "30K", "40K", "50K", "60K", "70K", "80K", "90K", "100K")) +
  scale_y_continuous(breaks=c(100000, 200000, 300000, 400000), labels=c("100K", "200K", "300K", "400K"))

g4 <- ggplot() + 
  geom_point(aes(x = dataset$AlternativeMetalSalesLast12Mo, y = dataset$TotalSalesLast12Mo), color = 'blue') +
  xlab('Last 12 Mo\'s Alt Metal $') + 
  ylab('Last 12 Mo\'s $') +
  scale_x_continuous(breaks=c(10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000), labels=c("10K", "20K", "30K", "40K", "50K", "60K", "70K", "80K", "90K", "100K")) +
  scale_y_continuous(breaks=c(100000, 200000, 300000, 400000), labels=c("100K", "200K", "300K", "400K"))

grid.arrange(g1, g2, g3, g4, ncol=2)

```

From the scatter plots above, we can see that Plain Bands and Design Bands seem to have the strongest correlation and Diamond Bands and the Alternative Metal Bands have the least.

### Distribution of Credit Limit

We also wanted to understand how the **Credit Limits** were distributed and if the distribution was Gaussian. After plotting a histogram we realized that the Credit Limit distribution was not Gaussian. 

```{r, echo=FALSE}
ggplot() + 
  geom_histogram(aes(dataset$CreditLimit), bins = 40) +
  scale_x_continuous(breaks=c(5000, 10000, 15000, 20000, 30000, 40000, 50000), labels=c("5K", "10K", "15K", "20K", "30K", "40K", "50K")) +
  ggtitle('Distribution of Credit Limit') +
  xlab('Credit Limit') + 
  ylab('Distribution')
```

### Relationship Between Avg Days of Pay and Credit Limits

Then we wanted to examine if there was a realtionship between the **Average Days of Pay Categories** and the **Credit Limit**.

```{r, echo=FALSE, warning=FALSE}
ggplot(dataset, aes(x = AvgDaysOfPayCategory, y = CreditLimit, fill = AvgDaysOfPayCategory)) + 
  geom_boxplot(outlier.color = 'blue', outlier.shape = 1) +
  scale_y_log10() +
  stat_summary(fun.y = mean, geom = "point", shape = 1, size = 1, color = "white") +
  theme(legend.position = "") +
  ggtitle('Avg Days of Pay vs Credit Limit') +
  xlab('Avg Days of Pay') + 
  ylab('Credit Limit')
```

If we remove the Category N/A from the plot, we do see that the means (white circles) do show a possible polynomial relationship. We also wanted to see what the distribution of **Credit Limits** looked like for each of the **Average Days of Pay**.

### Distribution of Credit Limit Among Avg Days of Pay

After plotting a histogram for the Credit Limit for each of the Average Days of Pay category, we did see that the first three categories had a gaussian distribution for the Credit Limits.

```{r, echo=FALSE, warning=FALSE}

ggplot(data = dataset, aes(x = CreditLimit)) + geom_histogram(binwidth = 2000) + facet_wrap(~AvgDaysOfPayCategory)

```

We have examined our dataset for near zero variance parameters and identified the following variables to be near zero:

```{r, echo=FALSE}

nzv <- nearZeroVar(dataset, saveMetrics= TRUE)
print(nzv[nzv$nzv,][1:5,])

```

We have observed that the near zero variance parameters were returns related and We did not further investigate the remaining Returns related variables in our dataset. 

## Non-Sales Attributes and Possible Correlations

We enhanced our dataset by bringing in Census data for **Retail Trade: Geographic Area Series: Summary Statistics for the U.S., States, Metro Areas, Counties, and Places**, **Annual Estimates of the Resident Population**, **MEDIAN INCOME IN THE PAST 12 MONTHS (IN 2016 INFLATION-ADJUSTED DOLLARS)**, **TOTAL POPULATION**, and **INDUSTRY BY MEDIAN EARNINGS IN THE PAST 12 MONTHS (IN 2016 INFLATION-ADJUSTED DOLLARS) FOR THE CIVILIAN EMPLOYED POPULATION 16 YEARS AND OVER** and mapped these values to our customer records via StateFIPS, CountyFIPS and CBSA Codes (for metro areas).

### Avg Days of Pay and Median Household Income 

We wanted to see if there was a correlation between the Average Days of Pay and Median Household Income where the stores are located and we did see that the first three categories had very similar patterns of Median Income distribution.

```{r, echo=FALSE, warning=FALSE}
ggplot(dataset, aes(x = AvgDaysOfPayCategory, y = MedianIncomeHouseholds, color = CreditLimit)) + 
  geom_jitter() +
  stat_summary(fun.y = mean, geom = "point", shape = 1, size = 2, color = "red") +
  theme(legend.position = "") +
  ggtitle('Avg Days of Pay vs Median Income Households') +
  xlab('Avg Days of Pay') + 
  ylab('Median Income Households')
```

And finally we wanted to examine if there was any kind of correlation between the ** Total Sales Last 12 Mos** and the **Urban Influence**. Urban Influence is an indicator which describes a statistical area whether it is a Major Metro Area, Micro Metro Area, Micro Metro Adjesent to a Major Metro Area, or more Rural.

Additionally, we have examined the correlation between the **Total Population** and the **Number of Jewelry Stores** (via the NAICS Code 4483 - Jewelry Stores) associated with that customer's location.

```{r, echo=FALSE, warning=FALSE}
g1 <- ggplot(dataset, aes(x = factor(UrbanInfluenceCode), y = dataset$TotalSalesLast12Mo, color = MetroIndicator)) + 
  geom_jitter() +
  scale_y_continuous(breaks=c(100000, 200000, 300000, 400000), labels=c("100K", "200K", "300K", "400K")) +
  stat_summary(fun.y = mean, geom = "point", shape = 1, size = 1, color = "white") +
  theme(legend.position = "") +
  ggtitle('12 Mo Sales Over Urban Influence') +
  xlab('Urban Influence Code') + 
  ylab('Total $ Last 12 Mo')

g2 <- ggplot() + 
  geom_point(aes(x = dataset$TotalPopulation, y = dataset$NumberOfJewelryStores), color = 'blue') +
  scale_x_continuous(breaks=c(5000000, 10000000, 15000000, 20000000), labels=c("5M", "10M", "15M", "20M")) +
  scale_y_continuous(breaks=c(1000, 2000, 3000, 4000, 5000), labels=c("1K", "2K", "3K", "4K", "5K")) +
  ggtitle('Total Population vs # Stores') +
  xlab('Total Population') + 
  ylab('# Jewelry Stores')

grid.arrange(g1, g2, ncol=2)

```

We did see that the larger Metro and Micro areas (light blue, Urban Influence Codes 1 and 2) did have customers with higher **Total SalesLast 12 Mos** and as the **Urban Influence** moved towards rural the total sales significantly became lower. Additionally, we saw a significant correlation between the **Total Population** and the **Number of Stores** from out dataset.

# Build a Linear Model 

Using all the variables (except JBT Rating), we examined revelance of the variables in our dataset. The reason we exluded the JBT Rating is because we want to be able to make predictions for the new customers who do not have a Credit Rating, so we did not want this variable to play a role in our model.

```{r, echo=FALSE}

model <- lm(formula = CreditLimit ~ SalesCurrentYr + SalesLastYr + PlainBandSalesLast12Mo + DesignBandSalesLast12Mo + DiamondBandSalesLast12Mo + AlternativeMetalSalesLast12Mo + TotalSalesLast12Mo + BuyingGroupMember + NumberOfStoreLocations + AvgDaysOfPayCategory + AvgDaysOfPay + MaxDaysPastDue + MaxAmtPastDue + AvgAmtPastDue + ReturnedPaymentCount + MaxReturnedPaymentAmt + RuralUrbanContinuumCode + UrbanInfluenceCode + MetroIndicator + TotalPopulation + MedianEarnings + MedianRetailTradeEarnings + TotalHouseholds + MedianIncomeHouseholds + NumberOfJewelryStores + NumberOfJewelryStoresState + JewelryStoreSalesState, data = dataset_train)

summary(model)

```

After viewing the summary and looking at the significance of the coefficients we notice tha the coefficient with the highest P value is the intercept.

### Remove the intercept

```{r, echo=FALSE}

model <- lm(formula = CreditLimit ~ 1 + SalesCurrentYr + SalesLastYr + PlainBandSalesLast12Mo + DesignBandSalesLast12Mo + DiamondBandSalesLast12Mo + AlternativeMetalSalesLast12Mo + TotalSalesLast12Mo + BuyingGroupMember + NumberOfStoreLocations + AvgDaysOfPayCategory + AvgDaysOfPay + MaxDaysPastDue + MaxAmtPastDue + AvgAmtPastDue + ReturnedPaymentCount + MaxReturnedPaymentAmt + RuralUrbanContinuumCode + UrbanInfluenceCode + MetroIndicator + TotalPopulation + MedianEarnings + MedianRetailTradeEarnings + TotalHouseholds + MedianIncomeHouseholds + NumberOfJewelryStores + NumberOfJewelryStoresState + JewelryStoreSalesState - 1, data = dataset_train)

summary(model)

```

Using **Backward Elimitaion**, we started going through the list of coefficients and removed the largest value coefficient one at a time and re-ran the model and the model summary.

### Backward Elimitaion to find the significant coefficients

After removing the coefficient the $R^2 Adjusted$ value went up significantly. Next coefficient in the list was the **Average Days of Pay Category** and even though the P values for the categorical coefficients were high/highest, removing this variable actually reduced the $R^2 Adjusted$ value of the model. Looking at our earlier graphs, we thought that there is some correlation between the Average Days of Pay Categories of 0-10 Days, 10-30 Days and 30-60 Days vs the Credit Limit so we have decided to keep this variable and remove the next lowest P value coefficient.

We went through the iteration of removing coefficients until we could no longer reduce the $R^2 Adjusted$ value.

```{r, echo=FALSE}
RSqrPerformance <- data.frame(
  Iterations = 1:22,
  Action = c('All Variables', 'Remove intercept', 'Remove AvgDaysOfPayCategory', 'Add AvgDaysOfPayCategory', 'Remove TotalHouseholds', 'Removed TotalPopulation', 'Remove MaxAmtPastDue', 'Remove AlternativeMetalSalesLast12Mo', 'Remove TotalSalesLast12Mo', 'Remove UrbanInfluenceCode', 'Remove MaxDaysPastDue', 'Remove DiamondBandSalesLast12Mo', 'Remove MaxReturnedPaymentAmt', 'Remove AvgDaysOfPay', 'Remove NumberOfJewelryStoresState', 'Remove JewelryStoreSalesState', 'Remove AvgAmtPastDue', 'Remove MedianRetailTradeEarnings', 'Remove PlainBandSalesLast12Mo', 'Remove NumberOfJewelryStores', 'Remove BuyingGroupMember', 'Remove ReturnedPaymentCount'),
  RSqrAdjValues = c(0.1201, 0.4704, 0.4455, 0.4704, 0.4708, 0.4713, 0.4717, 0.472, 0.4724, 0.4728, 0.4731, 0.4735, 0.4738, 0.474, 0.4743, 0.4746, 0.4749, 0.475, 0.4751, 0.4774, 0.4776, 0.4774)
)

print(RSqrPerformance)
```

Our final linear model became the following after our **Backward Elimitaion** process:

```{r}

model <- lm(formula = CreditLimit ~ SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data = dataset_train)

summary(model)

```

```{r}

predictions <- predict(model, dataset_train)

summary(predictions)

```
```{r, echo=FALSE, include = FALSE}
y <- dataset_train$CreditLimit
y_hat <- predict(model, dataset_train)

```

We calculated the $RMSE$ value for our **Training Set** predictions:

```{r, echo=FALSE}
# calculate Root Mean Squared error
rmse <- helper.RMSE(y, y_hat)
cat('Root Mean Square Error: ', rmse, "\n")

```
The $RMSE$ for our **Naive Model's** training set was 4312.198. So it looks like our new model improved slightly for our training set values.


Using our **Test Set**, we obtained a new set of predictions and calculated the $RMSE$ value for our test predictions:

```{r, echo=FALSE, include = FALSE}
y <- dataset_test$CreditLimit
y_hat <- predict(model, dataset_test)

```


```{r, echo=FALSE}
# calculate Root Mean Squared error
rmse <- helper.RMSE(y, y_hat)
cat('Root Mean Square Error: ', rmse, "\n")

```

The $RMSE$ for our **Naive Model's** test set was 4106.185. Based on these $RMSE$ values, our new model did not improve for our test set!

# Build a linear model #2.a

We came back to our linear model after we built our Decision-Tree model to see if we can build a similar linear model and if that would perform better. Using the same relevant variables which we have used in our decision tree as a start, and going through the Backward Elimination process, we came up with the following model:

```{r, echo=FALSE}

model <- lm(formula = CreditLimit ~ DesignBandSalesLast12Mo + AvgDaysOfPay + SalesLastYr + TotalSalesLast12Mo + AlternativeMetalSalesLast12Mo + MedianIncomeHouseholds + SalesCurrentYr + PlainBandSalesLast12Mo + MedianEarnings + RuralUrbanContinuumCode, data = dataset_train)

summary(model)

```

The $R^2 Adjusted$ value for this model was **significantly lower** than the model we came up with earlier!

# Build a linear model #2.b

We also wanted to examine if we could build a better linear model using non-sales variables. We started with all non-sales related variables which we have obtained from census.gov and went through the backward elimination and removed the least significant coefficient until we have reached an $R^2 Adjusted$ value which did not improve any further.

```{r, echo=FALSE}

model <- lm(formula = CreditLimit ~ RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds + NumberOfJewelryStores - 1, data = dataset_train)

summary(model)

```

This one was significantly better than the one we came up with using the variables from the Decision Tree model and our original **Naive** model, but it wasn't better than the one we developed using the Backward Elimination at the top of our analysis.

# Build Generalized Linear Model

We wanted to test and try different "family" parameters and other optimization parameters in our next iterations so we decided to verify if the results of the `lm()` function and the `glm()` function matched.

```{r, echo=FALSE}

model <- glm(CreditLimit ~ SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, family = gaussian, data=dataset_train)

summary(model)

```

```{r, echo=FALSE}

predictions <- predict(model, dataset_train)

summary(predictions)

```





