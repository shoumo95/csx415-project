---
title: "Incorporate Additional Data"
author:
  - name: Hakan Egeli
  - name: Soumyendu Sarkar
date: 'May 14, 2018'
knit: (function(inputFile, encoding) { 
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=file.path(dirname(inputFile), '../reports/', 'step-05.b-incorporate-addl-data.html')) })
output: html_document
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, root.dir = "..")
```


```{r, echo=FALSE, include = FALSE}

# ProjectTemplate will autoload the data and also create training and test datasets

cwd <- getwd()
setwd("..")

library('ProjectTemplate')
load.project()

setwd(cwd)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(e1071)
require(ggplot2)
require(gridExtra)
require(caret)
require(randomForest)

library(ROCR)
library(Hmisc)

# for multinom and nnet methods
library(nnet)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

# optional parallelism to be used by caret's trainControl 
library(doParallel);

cl <- makeCluster(detectCores())
registerDoParallel(cl)

```

### Additional Data

We have decided to add Retail Sales data from the U.S. Census Bureau, data set named **Retail Trade: Geographic Area Series: Summary Statistics for the U.S., States, Metro Areas, Counties, and Places: 2012 (EC1244A1)**. This dataset contains information on Number of Establishments, Sales, and Annual Payroll for various NAICS codes on economic census geographies (CBSACode).

```{}
data <- ECN.2012.US.44A1.with.ann
data <- data[data$NAICS.id=="448310",]

```

CBSA Code, core-based statistical area, is a U.S. geographic area defined by the Office of Management and Budget (OMB) that consists of one or more counties (or equivalents) anchored by an urban center of at least 10,000 people plus adjacent counties that are socioeconomically tied to the urban center by commuting.

We were primarily interested in the NAICS code 448310, Jewelry Stores and our original dataset already had CBSA Codes for each record. However, some of the entries for the Sales column in the EC1244A1 dataset contained a letter D instead of a numeric value in order to avoid disclosing data for individual companies. When we joined the two datasets we had to come up with a value to replece these "D" values.

```{}
# RCPTOT values for some of the statistical areas are masked with the letter D for privacy reasons.
# replace the D values with NA so that we can assign a numeric value to them
data <- data.frame(lapply(data, as.character), stringsAsFactors=FALSE)
data$RCPTOT = as.numeric(data$RCPTOT)

# get Urban Influence Codes to calculate missing store sales averages for each Influence group
data <- merge(data, CBSACodeUrbanInfluenceCode, by.x = "GEO.id2", by.y = "CBSACode", all.x = TRUE)

```

The standard approach here could have been replacing the "D" values with a $mean$ value for the Sales column, however the sales values from large cities would influence the mean in a way that this mean would be much higher than the largest sales value from some of the rural (non-urban) areas. Therefore, we have decided to map the CBSA codes to Urban Influence Codes (Urban Influence Codes form a classification scheme that distinguishes metropolitan counties by population size of their metro area, and nonmetropolitan counties by size of the largest city or town and proximity to metro and micropolitan areas, subdivided into two metro and 10 nonmetro categories, resulting in a 12-part county classification).

Joining the EC1244A1 dataset for the NAICS code 448310 withe the Urban Influence Codes gave us the ability to calculate means for each Urban Influence Code and we had planned to use these means to replace missing values for the sales numbers when we join the EC1244A1 datase to our original dataset.

```{}
# calculate the means for each of the Urban Influence Codes
UrbanInfluenceCode_means <- data %>%
  group_by(UrbanInfluenceCode) %>%
  summarise(mean_RCPTOT = mean(RCPTOT, na.rm = TRUE))

```

**Urban Influence Code Means for the EC1244A1 Dataset**

```{r, echo=FALSE, warning=FALSE}
UrbanInfluenceCode_means_start
```

When we looked at the means above, we did see that Urban Influence Codes 1 (Large-in a metro area with at least 1 million residents or more) and 2 (Small-in a metro area with fewer than 1 million residents) had a much larger mean compared to the non-metro Urban Influence Codes. We also noticed that codes 4, 6, 7, 9, 10, 11 and 12 were missing Sales data and we couldn't calculate a mean for those Urban Influence Codes. We have decided to take the mean of all non-metro sales records (excluding Urban Influence Codes 1 and 2) and use that mean to substitute for codes 4, 6, 7, 9, 10, 11 and 12. Our final mean substitution table became the following:

```{}
# however, there are some urban influence codes which has no data at all. These are primarily non-urban areas. 
# we will determine a non-urban mean and assign this mean to the other missing urban influence codes.
missing_mean <- mean(data$RCPTOT[data$UrbanInfluenceCode>2], na.rm = T)

# prepare a full list of Urban Influence codes and means for these codes
UrbanInfluenceCode_means <- CustomerData %>% 
  select(UrbanInfluenceCode) %>% 
  distinct() %>% 
  arrange(UrbanInfluenceCode) %>%
  left_join(UrbanInfluenceCode_means, by="UrbanInfluenceCode", all.x = TRUE)

# and also set the missing_mean to the urband influence codes with NA values
UrbanInfluenceCode_means$mean_RCPTOT[is.na(UrbanInfluenceCode_means$mean_RCPTOT)] <- missing_mean

```

**Final Urban Influence Code Means for the EC1244A1 Dataset**

```{r, echo=FALSE, warning=FALSE}
UrbanInfluenceCode_means
```

We substitued the missing Sales data in the EC1244A1 dataset (RCPTOT) with the mean_RCPTOT which we had calculated and then performed a join to our dataset and added this new column.

```{}
# add the RCPTOT field to the CustomerData dataset using a left join
merged <- merge(CustomerData, data[,c("GEO.id2", "RCPTOT")], by.x = "CBSACode", by.y = "GEO.id2", all.x = TRUE)

# and bring the UrbanInfluenceCode_means to the merged dataset so that we can use these means to substitute the missing RCPTOT
merged <- merged %>% left_join(UrbanInfluenceCode_means, by="UrbanInfluenceCode", all.x = TRUE)

# if RCPTOT is NA use the UrbanInfluenceCode_means, else keep it
merged$RCPTOT = ifelse(is.na(merged$RCPTOT), merged$mean_RCPTOT, merged$RCPTOT)

```

To see the impact of the new column we have tested it against the two top performing models that we had come up so far and reran our cross validations once without the new field and once with the new field for both of the models (Random Forest and Multinomial Regression Model via Neural Networks).

### Adding the Additional Data Field to Two Top Performing Models

**15 Fold Cross-Validation Accuracy and Kappa Results**

```{r, echo=FALSE, warning=FALSE}

metric.create <- function(){
  setNames(data.frame(matrix(ncol = 4, nrow = 0)), c("Accuracy", "AccuracySD", "Kappa", "KappaSD"))
}

metric.append <- function(df, model, row_name){
  df_tmp <- metric.create()
  df_tmp <- bind_rows(df_tmp, c(Accuracy=mean(model$resample[["Accuracy"]]),
                                AccuracySD=sd(model$resample[["Accuracy"]]),
                                Kappa=mean(model$resample[["Kappa"]]),
                                KappaSD=sd(model$resample[["Kappa"]])))
  rownames(df_tmp) <- c(row_name)
  rbind(df, df_tmp)
}

df <- metric.create()

```

```{r, echo=FALSE, warning=FALSE}

control <- trainControl(method="repeatedcv", number=5, repeats=3, classProbs=TRUE, allowParallel=TRUE)

```

```{r, echo=FALSE, warning=FALSE}

model_rf <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="rf", metric="Accuracy", trControl=control, tuneGrid=data.frame(.mtry=19))

df <- metric.append(df, model_rf, "rf")

```

```{r, echo=FALSE, warning=FALSE}

model_rf_addl <- train(CreditLimitCategory ~ RCPTOT + JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="rf", metric="Accuracy", trControl=control, tuneGrid=data.frame(.mtry=19))

df <- metric.append(df, model_rf_addl, "rf w/ RCPTOT")

```

```{r, echo=FALSE, warning=FALSE}

model_multinom <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="multinom", metric="Accuracy", trControl=control, maxit=600, trace=FALSE)

df <- metric.append(df, model_multinom, "multi")

```

```{r, echo=FALSE, warning=FALSE}

model_multinom_addl <- train(CreditLimitCategory ~ RCPTOT + JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="multinom", metric="Accuracy", trControl=control, maxit=600, trace=FALSE)

df <- metric.append(df, model_multinom_addl, "multi w/ RCPTOT")

```

```{r, echo=FALSE, warning=FALSE}

# display the metric for Accuracy and Kappa values
df[with(df, order(-Accuracy)),]

```

```{r, echo=FALSE, warning=FALSE}
data <- data.frame(rf=model_rf$resample$Accuracy,
                   rf_RCPTOT=model_rf_addl$resample$Accuracy,
                   multinom=model_multinom$resample$Accuracy,
                   multinom_RCPTOT=model_multinom_addl$resample$Accuracy)
data <- gather(data, model, accuracy, rf:multinom_RCPTOT)

```

```{r, echo=FALSE, warning=FALSE}

ggplot(data, aes(x = model, y = accuracy, fill = model)) + 
  stat_summary(fun.y = mean, geom = "point") +
  stat_summary(fun.data = mean_sdl, geom = "errorbar", width = 0.1) +
  theme(legend.position = "") +
  ggtitle('Accuracy Confidence Interval') +
  xlab('Model') + 
  ylab('Accuracy')

```

```{r, echo=FALSE, warning=FALSE}
data <- data.frame(rf=model_rf$resample$Kappa,
                   rf_RCPTOT=model_rf_addl$resample$Kappa,
                   multinom=model_multinom$resample$Kappa,
                   multinom_RCPTOT=model_multinom_addl$resample$Kappa)
data <- gather(data, model, kappa, rf:multinom_RCPTOT)

```

```{r, echo=FALSE, warning=FALSE}

ggplot(data, aes(x = model, y = kappa, fill = model)) + 
  stat_summary(fun.y = mean, geom = "point") +
  stat_summary(fun.data = mean_sdl, geom = "errorbar", width = 0.1) +
  theme(legend.position = "") +
  ggtitle('Kappa Confidence Interval') +
  xlab('Model') + 
  ylab('Kappa')

```

**Test Set True Positive Rate vs False Positive Rate of rf w/o RCPTOT**

```{r, echo=FALSE, warning=FALSE, message=FALSE}

predictions <- predict(model_rf, newdata=dataset_test, type = "prob")

```

```{r, echo=FALSE}
df <- setNames(data.frame(matrix(ncol = 5, nrow = 0)), c("class_name", "x.values", "y.values", "x.name", "y.name"))

classifications <- as.character(model_rf$levels)

for (classification in classifications) {

  pred <- prediction(predictions[, classification], ifelse(dataset_test$CreditLimitCategory == classification, 1, 0)) 
  perf <- performance(pred, "tpr", "fpr")

  df <- bind_rows(df,tibble(class_name=classification,
               x.values=perf@x.values[[1]],
               y.values=perf@y.values[[1]],
               x.name=perf@x.name,
               y.name=perf@y.name))
}

ggplot(df) + 
  geom_point(aes(x = x.values, y = y.values)) +
  facet_wrap("class_name") + 
  xlab(perf@x.name) + 
  ylab(perf@y.name)

```

**Test Set True Positive Rate vs False Positive Rate of rf w/ RCPTOT**

```{r, echo=FALSE, warning=FALSE, message=FALSE}

predictions <- predict(model_rf_addl, newdata=dataset_test, type = "prob")

```

```{r, echo=FALSE}
df <- setNames(data.frame(matrix(ncol = 5, nrow = 0)), c("class_name", "x.values", "y.values", "x.name", "y.name"))

classifications <- as.character(model_rf$levels)

for (classification in classifications) {

  pred <- prediction(predictions[, classification], ifelse(dataset_test$CreditLimitCategory == classification, 1, 0)) 
  perf <- performance(pred, "tpr", "fpr")

  df <- bind_rows(df,tibble(class_name=classification,
               x.values=perf@x.values[[1]],
               y.values=perf@y.values[[1]],
               x.name=perf@x.name,
               y.name=perf@y.name))
}

ggplot(df) + 
  geom_point(aes(x = x.values, y = y.values)) +
  facet_wrap("class_name") + 
  xlab(perf@x.name) + 
  ylab(perf@y.name)

```

### Conclusion

From the results above, we did not see a significant improvement in the Accuracy or the Kappa values. The best perfoming model (rf) and the other model (multinomial) perfomed slightly better. The only noticable change was in the confidence intervals, the interval got smaller for the rf model (due to decrease in standard deviation) and the interval got bigger for the multinomial.

```{r, echo=FALSE, include=FALSE}

# save the model as a file to be used in the model package
#saveRDS(model_rf, "../data/final_model.rds")

# save the test data to be included in the model package
#save(dataset_test, file = "../data/creditlimittestdata.rda")

```

```{r, echo=FALSE}

stopCluster(cl)

```
