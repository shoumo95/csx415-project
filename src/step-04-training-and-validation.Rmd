---
title: "Model Training And Validation"
author:
  - name: Hakan Egeli
  - name: Soumyendu Sarkar
date: 'May 8, 2018'
knit: (function(inputFile, encoding) { 
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=file.path(dirname(inputFile), '../reports/', 'step-04-training-and-validation.html')) })
output: html_document
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, root.dir = "..")
```


```{r, echo=FALSE, include = FALSE}

# ProjectTemplate will autoload the data and also create training and test datasets

cwd <- getwd()
setwd("..")

library('ProjectTemplate')
load.project()

setwd(cwd)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(e1071)
require(ggplot2)
require(gridExtra)
require(caret)
require(randomForest)

library(ROCR)
library(Hmisc)

# for multinom and nnet methods
library(nnet)

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

# optional parallelism to be used by caret's trainControl 
library(doParallel);

cl <- makeCluster(detectCores())
registerDoParallel(cl)

```

## Regression Models

```{r, echo=FALSE, warning=FALSE}

metric.regression.create <- function(){
  setNames(data.frame(matrix(ncol = 4, nrow = 0)), c("RMSE", "RMSESD", "Rsquared", "RsquaredSD"))
}

metric.regression.append <- function(df, model, row_name){
  df_tmp <- metric.regression.create()
  df_tmp <- bind_rows(df_tmp, c(RMSE=mean(model$resample[["RMSE"]]),
                                RMSESD=sd(model$resample[["RMSE"]]),
                                Rsquared=mean(model$resample[["Rsquared"]]),
                                RsquaredSD=sd(model$resample[["Rsquared"]])))
  rownames(df_tmp) <- c(row_name)
  rbind(df, df_tmp)
}

df <- metric.regression.create()

```

```{r, echo=FALSE, warning=FALSE}

control <- trainControl(method="repeatedcv", number=5, repeats=2)

```

```{r, echo=FALSE, warning=FALSE}

# generalized linear naive model only using the Sales Last Year
model_glm_naive <- train(CreditLimit ~ SalesLastYr, data=dataset_train, method="glm", metric="RMSE", trControl=control)

df <- metric.regression.append(df, model_glm_naive, "glm naive")

```

```{r, echo=FALSE, warning=FALSE}

# generalized linear model with the significant independent variables, with the intercept
model_glm_int <- train(CreditLimit ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds, data=dataset_train, method="glm", metric="RMSE", trControl=control)

df <- metric.regression.append(df, model_glm_int, "glm w/ intercept")

```

```{r, echo=FALSE, warning=FALSE}

# generalized linear model with the significant independent variables, without the intercept
model_glm <- train(CreditLimit ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="glm", metric="RMSE", trControl=control)

df <- metric.regression.append(df, model_glm, "glm w/o intercept")

```

Comparison of linear models using the RMSE and $R^2$ with their respective standard deviations.

```{r, echo=FALSE, warning=FALSE}

# display the metric for RMSE and Rsquared values
df[with(df, order(RMSE)),]

```

## Regression Tests Accuracy

RMSE, $R^2$ and MAE results using the test dataset.

```{r, warning=FALSE}

predictions <- predict(model_glm, newdata=dataset_test)
postResample(dataset_test$CreditLimit, predictions)

```


## Classification Models

```{r, echo=FALSE, warning=FALSE}

metric.create <- function(){
  setNames(data.frame(matrix(ncol = 4, nrow = 0)), c("Accuracy", "AccuracySD", "Kappa", "KappaSD"))
}

metric.append <- function(df, model, row_name){
  df_tmp <- metric.create()
  df_tmp <- bind_rows(df_tmp, c(Accuracy=mean(model$resample[["Accuracy"]]),
                                AccuracySD=sd(model$resample[["Accuracy"]]),
                                Kappa=mean(model$resample[["Kappa"]]),
                                KappaSD=sd(model$resample[["Kappa"]])))
  rownames(df_tmp) <- c(row_name)
  rbind(df, df_tmp)
}

df <- metric.create()

```

```{r, echo=FALSE, warning=FALSE}

control <- trainControl(method="repeatedcv", number=7, repeats=1, classProbs=TRUE, allowParallel=TRUE)

```

```{r, echo=FALSE, warning=FALSE}

model_naive <- train(CreditLimitCategory ~ SalesLastYr, data=dataset_train, method="rpart", metric="Accuracy", trControl=control)

df <- metric.append(df, model_naive, "naive")

```

```{r, echo=FALSE, warning=FALSE}

model_rpart <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="rpart", metric="Accuracy", trControl=control)

df <- metric.append(df, model_rpart, "rpart")

```

```{r, echo=FALSE, warning=FALSE}

model_rpart2 <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="rpart2", metric="Accuracy", trControl=control)

df <- metric.append(df, model_rpart2, "rpart2")

```

```{r, echo=FALSE, warning=FALSE, message=FALSE}

model_steplda <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="stepLDA", metric="Accuracy", trControl=control, trace=FALSE)

df <- metric.append(df, model_steplda, "steplda")

```

```{r, echo=FALSE, warning=FALSE}

model_svmLinear <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="svmLinear", metric="Accuracy", trControl=control)

df <- metric.append(df, model_svmLinear, "svmLinear")

```

```{r, echo=FALSE, warning=FALSE}

model_svmRad <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="svmRadial", metric="Accuracy", trControl=control)

df <- metric.append(df, model_svmRad, "svmRad")

```

```{r, echo=FALSE, warning=FALSE}

model_svmPoly <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="svmPoly", metric="Accuracy", trControl=control)

df <- metric.append(df, model_svmPoly, "svmPoly")

```

```{r, echo=FALSE, warning=FALSE}

model_rf <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="rf", metric="Accuracy", trControl=control, tuneGrid=data.frame(.mtry=19))

df <- metric.append(df, model_rf, "rf")

```

```{r, echo=FALSE, warning=FALSE}

model_multinom <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="multinom", metric="Accuracy", trControl=control, maxit=600, trace=FALSE)

df <- metric.append(df, model_multinom, "multi")

```

```{r, echo=FALSE, warning=FALSE}

model_avNNet <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="avNNet", metric="Accuracy", trControl=control, maxit=600, trace=FALSE)

df <- metric.append(df, model_avNNet, "avNNet")

```

```{r, echo=FALSE, warning=FALSE}

model_nnet <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset_train, method="nnet", trControl=control, tuneGrid=data.frame(.size=20, .decay=1.0e-1), rang=0.1, maxit=10000, trace=FALSE)

df <- metric.append(df, model_nnet, "nnet")

```

To improve our initial Naive model we have built several more models for our classification problem using:

* Decision Tree (**rpart** and **raprt2**),
* Linear Discriminant Analysis with Stepwise Feature Selection (**stepLDA**),
* Neural Networks Using Model Averaging (**avNNet**),
* Multinomial Regression Model via Neural Networks (**multinom**),
* Random Forest (**rf**),
* Single-hidden-layer Neural Network (**nnet**),
* Support Vector Machines with Linear Kernel (**svmLinear**)
* Support Vector Machines with Radial Basis Function Kernel (**svmRad**)
* Support Vector Machines with Polynomial Kernel (**svmPoly**)

and we have performed a repeated K-fold cross-validation totaling 10 folds for each model.


```{r, echo=FALSE, warning=FALSE}

# display the metric for Accuracy and Kappa values
df[with(df, order(-Accuracy)),]

```

```{r, echo=FALSE, warning=FALSE}
data <- data.frame(naive=model_naive$resample$Accuracy,
                   rpart=model_rpart$resample$Accuracy,
                   rpart2=model_rpart2$resample$Accuracy,
                   steplda=model_steplda$resample$Accuracy,
                   svmLinear=model_svmLinear$resample$Accuracy,
                   svmRad=model_svmRad$resample$Accuracy,
                   svmPoly=model_svmPoly$resample$Accuracy,
                   rf=model_rf$resample$Accuracy,
                   multinom=model_multinom$resample$Accuracy,
                   avNNet=model_avNNet$resample$Accuracy,
                   nnet=model_nnet$resample$Accuracy)
data <- gather(data, model, accuracy, naive:nnet)

```

```{r, echo=FALSE, warning=FALSE}

ggplot(data, aes(x = model, y = accuracy, fill = model)) + 
  stat_summary(fun.y = mean, geom = "point") +
  stat_summary(fun.data = mean_sdl, geom = "errorbar", width = 0.1) +
  theme(legend.position = "") +
  ggtitle('Accuracy Confidence Interval') +
  xlab('Model') + 
  ylab('Accuracy')

```

```{r, echo=FALSE, warning=FALSE}
data <- data.frame(naive=model_naive$resample$Kappa,
                   rpart=model_rpart$resample$Kappa,
                   rpart2=model_rpart2$resample$Kappa,
                   steplda=model_steplda$resample$Kappa,
                   svmLinear=model_svmLinear$resample$Kappa,
                   svmRad=model_svmRad$resample$Kappa,
                   svmPoly=model_svmPoly$resample$Kappa,
                   rf=model_rf$resample$Kappa,
                   multinom=model_multinom$resample$Kappa,
                   avNNet=model_avNNet$resample$Kappa,
                   nnet=model_nnet$resample$Kappa)
data <- gather(data, model, kappa, naive:nnet)

```

```{r, echo=FALSE, warning=FALSE}

ggplot(data, aes(x = model, y = kappa, fill = model)) + 
  stat_summary(fun.y = mean, geom = "point") +
  stat_summary(fun.data = mean_sdl, geom = "errorbar", width = 0.1) +
  theme(legend.position = "") +
  ggtitle('Kappa Confidence Interval') +
  xlab('Model') + 
  ylab('Kappa')

```

We looked at the Accuracy and Kappa model performance metrics and the confidence interval for each model and determined that the Random Forest model and the Multinomial Regression model were the top two performers. Then, we cheched the performance metics for these two top models against the test dataset that we have set aside (which as not used during the training) to re-evaluate their performance.

## Classification Test Accuracy for the Top Two Models

Random Rorest model against the test data:

```{r, warning=FALSE}

predictions <- predict(model_rf, newdata=dataset_test)
cm <- confusionMatrix(dataset_test$CreditLimitCategory, predictions)
cm$overall

```
```{r, echo=FALSE}

cm$byClass[,c("Sensitivity", "Specificity", "Precision", "Recall")]

```

```{r, echo=FALSE, warning=FALSE}

ggplot(data = data.frame(cm$table), aes(x=Prediction, y=Reference)) +
  geom_tile(aes(fill=Freq)) + 
  scale_fill_gradient(low = "white", high = "steelblue") +
  theme(axis.text.x=element_text(angle=90,hjust=1)) +
  xlab("Predictions") +
  ylab("Ground Truth (Observations)") +
  geom_text(aes(label = Freq), size = 2) +
  ggtitle("Random Forest Confusion Matrix")

```
**ROC Curves**

```{r, echo=FALSE, warning=FALSE, message=FALSE}

predictions <- predict(model_rf, newdata=dataset_test, type = "prob")

```

```{r, echo=FALSE}
df <- setNames(data.frame(matrix(ncol = 5, nrow = 0)), c("class_name", "x.values", "y.values", "x.name", "y.name"))

classifications <- as.character(model_rf$levels)

for (classification in classifications) {

  pred <- prediction(predictions[, classification], ifelse(dataset_test$CreditLimitCategory == classification, 1, 0)) 
  perf <- performance(pred, "tpr", "fpr")

  df <- bind_rows(df,tibble(class_name=classification,
               x.values=perf@x.values[[1]],
               y.values=perf@y.values[[1]],
               x.name=perf@x.name,
               y.name=perf@y.name))
}

ggplot(df) + 
  geom_point(aes(x = x.values, y = y.values)) +
  facet_wrap("class_name") + 
  xlab(perf@x.name) + 
  ylab(perf@y.name)

```

```{r, echo=FALSE}
df <- setNames(data.frame(matrix(ncol = 5, nrow = 0)), c("class_name", "x.values", "y.values", "x.name", "y.name"))

classifications <- as.character(model_rf$levels)

for (classification in classifications) {

  pred <- prediction(predictions[, classification], ifelse(dataset_test$CreditLimitCategory == classification, 1, 0)) 
  perf <- performance(pred, "sens", "spec")

  df <- bind_rows(df,tibble(class_name=classification,
               x.values=perf@x.values[[1]],
               y.values=perf@y.values[[1]],
               x.name=perf@x.name,
               y.name=perf@y.name))
}

ggplot(df) + 
  geom_point(aes(x = x.values, y = y.values)) +
  facet_wrap("class_name") + 
  xlab(perf@x.name) + 
  ylab(perf@y.name)

```


Multinomial Regression model against the test data:

```{r, warning=FALSE}

predictions <- predict(model_multinom, newdata=dataset_test)
cm <- confusionMatrix(dataset_test$CreditLimitCategory, predictions)
cm$overall

```

```{r, echo=FALSE}

cm$byClass[,c("Sensitivity", "Specificity", "Precision", "Recall")]

```

```{r, echo=FALSE, warning=FALSE}

ggplot(data = data.frame(cm$table), aes(x=Prediction, y=Reference)) +
  geom_tile(aes(fill=Freq)) + 
  scale_fill_gradient(low = "white", high = "steelblue") +
  theme(axis.text.x=element_text(angle=90,hjust=1)) +
  xlab("Predictions") +
  ylab("Ground Truth (Observations)") +
  geom_text(aes(label = Freq), size = 2) +
  ggtitle("Multinomial Regression Confusion Matrix")

```

**ROC Curves**

```{r, echo=FALSE, warning=FALSE, message=FALSE}

predictions <- predict(model_multinom, newdata=dataset_test, type = "prob")

```

```{r, echo=FALSE}
df <- setNames(data.frame(matrix(ncol = 5, nrow = 0)), c("class_name", "x.values", "y.values", "x.name", "y.name"))

classifications <- as.character(model_multinom$levels)

for (classification in classifications) {

  pred <- prediction(predictions[, classification], ifelse(dataset_test$CreditLimitCategory == classification, 1, 0)) 
  perf <- performance(pred, "tpr", "fpr")

  df <- bind_rows(df,tibble(class_name=classification,
               x.values=perf@x.values[[1]],
               y.values=perf@y.values[[1]],
               x.name=perf@x.name,
               y.name=perf@y.name))
}

ggplot(df) + 
  geom_point(aes(x = x.values, y = y.values)) +
  facet_wrap("class_name") + 
  xlab(perf@x.name) + 
  ylab(perf@y.name)

```

```{r, echo=FALSE}
df <- setNames(data.frame(matrix(ncol = 5, nrow = 0)), c("class_name", "x.values", "y.values", "x.name", "y.name"))

classifications <- as.character(model_multinom$levels)

for (classification in classifications) {

  pred <- prediction(predictions[, classification], ifelse(dataset_test$CreditLimitCategory == classification, 1, 0)) 
  perf <- performance(pred, "sens", "spec")

  df <- bind_rows(df,tibble(class_name=classification,
               x.values=perf@x.values[[1]],
               y.values=perf@y.values[[1]],
               x.name=perf@x.name,
               y.name=perf@y.name))
}

ggplot(df) + 
  geom_point(aes(x = x.values, y = y.values)) +
  facet_wrap("class_name") + 
  xlab(perf@x.name) + 
  ylab(perf@y.name)

```

## Final Model

From the last two model tests, we have confirmed that the model built using the random forest was the one with the best accuracy and the best Kappa performance metrics.

We re-trainned the Random Forest model using the entire dataset in order to save this model and package it.

```{r}

model_rf <- train(CreditLimitCategory ~ JBTRating + CreditLimitLocked + SalesCurrentYr + SalesLastYr + DesignBandSalesLast12Mo + NumberOfStoreLocations + AvgDaysOfPayCategory + ReturnedPaymentCount + RuralUrbanContinuumCode + MetroIndicator + MedianEarnings + MedianIncomeHouseholds - 1, data=dataset, method="rf", metric="Accuracy", trControl=control, tuneGrid=data.frame(.mtry=19))

```

Final Accuracy and Kappa values and Standard Deviations

```{r, echo=FALSE, warning=FALSE}

model_rf$results

```


```{r, echo=FALSE, include=FALSE}

# save the model as a file to be used in the model package
#saveRDS(model_rf, "../data/final_model.rds")

# save the test data to be included in the model package
#save(dataset_test, file = "../data/creditlimittestdata.rda")

```

```{r, echo=FALSE}

stopCluster(cl)

```
