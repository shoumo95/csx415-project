---
title: "Model Performance Evaluation"
author:
  - name: Hakan Egeli
  - name: Soumyendu Sarkar
date: 'April 17, 2018'
knit: (function(inputFile, encoding) { 
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=file.path(dirname(inputFile), '../reports/', 'model-performance-evaluation.html')) })
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, root.dir = "..")
```

## Project Setup

We chose to go with the ProjectTemplate package and structured our project accordingly. With this setup, our data gets automatically loaded from the data directory and using the munge feature we also automatically build a **training set** and a **test set** by dividing the original dataset using a 80/20 ratio. To make sure that both the training and the test sets have similar distributions, we used the `createDataPartition()` function and split the data over the **CreditLimit** variable.

We also set the `seed` to a fixed number so that we can produce the same sequence of random numbers between runs and our results are not effected by the changes in the datasets.

```{r, echo=FALSE, include = FALSE}

cwd <- getwd()

# load the project. This will autoload the data and also create training and test datasets
setwd("..")
library('ProjectTemplate')
load.project()

setwd(cwd)

```

```{r, echo=FALSE, include = FALSE}
#
# This function allows multiple plots to be displayed in columns and rows 
#
# Credit Cookbook for R (http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/)
#

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

## Naive Model

Instead of using a mean value for the credit limit for our Naive model, we chose Last Year&apos;s Sales (SalesLastYr) independent variable to predict Credit Limits. Our goal is to see whether we can formulate the Credit Limit with a simple equation as the following and whether this realtionship is good enough to get acceptable predictions for Credit Limits:

$$Credit Limit = w*Previous Years Sales+b$$ 

```{r}
model <- lm(formula = CreditLimit ~ SalesLastYr, data = dataset_train)
```


```{r, echo=FALSE}
modelSummary <-summary(model)

print(modelSummary)
```

The **summary** results of this model trained using the **training set** shows that both the intercept (b) and the coefficient (w) are significant (P values are much less than 0.05). This might be a good start but how good of a fit is this model to our data?

R-squared paramater indicates the the fitness of a model and we want this value to be closer to 1, aka we want the ratio of the sum of the square of the residuals ($SS_r$) over the sum of the square of the averages ($SS_t$) to be small so that $R^2$ becomes larger.

$$R^2 = 1 - SS_r/SS_t$$ 

The value `r modelSummary$r.squared` for the $R^2$ seemed low which is indicating that the model is not a good fit. And at this time we are not concerned with the Adjusted $R^2$ since we have only one independent variable in our model.

The three perfomance metrics, $R^2$, Adjusted $R^2$ and the RMSE, for the **training set** are:

```{r, echo=FALSE, include = FALSE}
y <- dataset_train$CreditLimit
y_hat <- predict(model, dataset_train)

```

```{r, echo=FALSE}
# calculate R-squared
R2 <- helper.RSquared(y, y_hat)
cat('R Squared: ', R2, "\n")

# calculate Adjusted R-squared
R2Adj <- helper.RSquaredAdjsuted(R2, length(y), 1)
cat('R Squared (Adjusted): ', R2Adj, "\n")

# calculate Root Mean Squared error
rmse <- helper.RMSE(y, y_hat)
cat('Root Mean Square Error: ', rmse, "\n")

```

```{r, echo=FALSE, include = FALSE}

cat('Abs diff between summary$r.squared and RSquared() is less than 1e-10:', abs(modelSummary$r.squared - R2) < 1e-10 , '\n' )

cat('Abs diff between summary$adj.r.squared and RSquaredAdjsuted() is less than 1e-10:', abs(modelSummary$adj.r.squared - R2Adj) < 1e-10 , '\n' )

```

And same metrics for the **test set** are:

```{r, echo=FALSE, include = FALSE}
y <- dataset_test$CreditLimit
y_hat <- predict(model, dataset_test)

```

```{r, echo=FALSE}
# calculate R-squared
R2 <- helper.RSquared(y, y_hat)
cat('R Squared: ', R2, "\n")

# calculate Adjusted R-squared
R2Adj <- helper.RSquaredAdjsuted(R2, length(y), 1)
cat('R Squared (Adjusted): ', R2Adj, "\n")

# calculate Root Mean Squared error
rmse <- helper.RMSE(y, y_hat)
cat('Root Mean Square Error: ', rmse, "\n")

```

The $R^2$ values are very close to eachother for the test set than the training set but in both cases these values still indicate not a good fit. The RSME values for both the raining and the test sets seems to be close to each other as well and we hope to see these numbers (RSME) go lower and the $R^2$ values go higher as we build better models.

## Interpretation of the Model and the Data

We have performed a preliminary data analysis and we began our examination of the data with the **Sales** numbers for the last calender year. We looked at the min, max, std dev and the quintiles to understand the spread of the data as well as to be able to do a comparison between the training and the test sets.

#### Training Set Summary

```{r, echo=FALSE}
summary(dataset_train$SalesLastYr)
```

```{r, echo=FALSE}
cat('Std Dev.', sd(dataset_train$SalesLastYr))
```

#### Test Set Summary

```{r, echo=FALSE}
summary(dataset_test$SalesLastYr)
```
```{r, echo=FALSE}
cat('Std Dev.', sd(dataset_test$SalesLastYr))
```

Both in the training set and the test set these numbers seemed to be close to each other and we do notice that the Sales numbers are clustred more towards the left of the scale. To visulaize this better, we have created a histogram of the sales numbers for the traning set and the test set so that we can do a side by side analysis.

#### Comparison of Distribution of Sales Between the Training Set and the Test Set

```{r, echo=FALSE}
hist_train <- ggplot() +
  geom_histogram(aes(dataset_train$SalesLastYr), bins = 40) +
  ggtitle('Last Year\'s Sales - Training Set') +
  xlab('Sales') +
  ylab('Count')

hist_test <- ggplot() +
  geom_histogram(aes(dataset_test$SalesLastYr), bins = 40) +
  ggtitle('Last Year\'s Sales - Test Set') +
  xlab('Sales') +
  ylab('Count')
```


```{r, echo=FALSE}
multiplot(hist_train, hist_test, cols=2)
```

Upon visual inspection of the **Sales** data between the training and the test sets, we do see that they are distributed similarly.

We also wanted to see how the distrubution of Last Year&apos;s Sales looked like compared to the distribution of Credit Limits for the same data points.

```{r, echo=FALSE}
ggplot() +
  geom_density(aes(dataset_train$SalesLastYr), color = 'red') +
  ggtitle('Distribution of Sales - Training Set') +
  xlab('Last Year\'s Sales') +
  ylab('Density')
```


```{r, echo=FALSE}
ggplot() +
  geom_density(aes(dataset_train$CreditLimit), color = 'red') +
  ggtitle('Distribution of Credit Limit - Training Set') +
  xlab('Credit Limit') +
  ylab('Density')
```

The Sales data looks like a normal distribution and we were hoping to see the same for the plot of the Credit Limits for our tranining data.

## Performance of the Model

To better understand the correlation between these two variables and how our model repsresents this data, we have plotted the **Credit Limit vs. Last Year&apos;s Sales** and also plotted a line representing our model.

```{r, echo=FALSE}
ggplot() + 
  geom_point(aes(x = dataset_train$SalesLastYr, y = dataset_train$CreditLimit), color = 'green') +
  geom_point(aes(x = dataset_test$SalesLastYr, y = dataset_test$CreditLimit), color = 'blue') +
  geom_line(aes(x = dataset$SalesLastYr, y = predict(model, newdata = dataset)), color = 'red') +
  ggtitle('Last Year Sales vs Credit Limit Compared to Training and Test Sets') +
  xlab('Last Year\'s Sales') + 
  ylab('Credit Limit')
```

From the plot of the data Credit Limit vs the Last Year&apos;s Sales, we do see that the two variables are not correlated and that is why our linear model is not performing well.

It is possible that the Credit Limit cannot be explained by one varaible and a linear relationship might still exist using other independent variables. It is also possible that the relationships are non-linear and we might have to try non-linear models to explain the data.

## Next Step

We will try to improve our linear model by examining all other variables that we have in our dataset and use a **Backward Elimitaion** to find out the most significant variables which might help us to build a better model. As we add more variables to describe our model we will also utilize the $Adjusted R^2$ predictor to account for the impact of the added variables to the $R^2$ value.

Future steps will also include adding non-linear terms to the model or building other classification models such as one which uses a **Decision Tree** or take advantage of ensemble learning by building a model using **Random Forest**.

We will evaluate the performance of these models against our performance metrics which we have established in the FPS document.


