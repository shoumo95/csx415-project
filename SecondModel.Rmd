---
title: "Second Model"
author:
  - name: Hakan Egeli
  - name: Soumyendu Sarkar
date: 'April 21, 2018'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=FALSE, include = FALSE}

# load the project. This will autoload the data and also create training and test datasets
library('ProjectTemplate')
load.project()

```

```{r, echo=FALSE, include=FALSE}

library(rpart)
library(rattle)
library(caret)
#library(reshape2) needed for mesh but no longer using it

```


## Perform some explortory data analysis.

- we looked at the dtree with all variables
- we choos the top variables
- we did iterated the dtree

- we took the same variables and reiterated out lm

- we took out the sales data to ceate an alternate model which does not depend on sales information

- Median Income and Median Income households which we obtained **census data** played a significant role

## Definitely plot (histogram) any response variables.
##Plot at your predictors.

d-tree plots

multiple univariable plots for lm

##Begin the analysis:

##For analysis/explanatory/exploratory projects start by

##For ML projects

##Build a linear model use lm for regression and/or glm( family=binomial. ... ) for classification

##Evaluate your linear model using your model evaluation RMarkdown document, if you are clever you can reuse the template and version it very quickly.

##Knit this to model-performance-linear.[html|pdf|md] where you are keeping your assets

##Make sure you know who to interpret a linear model! Let me know if we have to cover this.

##Build a tree model (rpart)

##plot the tree paying particular attention to the first few nodes, do they make sense?

##Evaluate your tree with you model evaluation RMarkdown document. Knit this to model-performance-rpart.[html|pdf|md]

##Has your model(s) met the model success criteria?




```{r, echo=FALSE}
ggplot(dataset, aes(x = AvgDaysOfPayCategory, y = MedianIncomeHouseholds, fill = AvgDaysOfPayCategory)) + 
  geom_boxplot(outlier.color = 'blue', outlier.shape = 1) +
  scale_y_log10() +
  stat_summary(fun.y = mean, geom = "point", shape = 1, size = 1, color = "white") +
  theme(legend.position = "") +
  ggtitle('Avg Days of Pay vs Credit Limit') +
  xlab('Avg Days of Pay') + 
  ylab('MedianIncomeHouseholds')
```

```{r, echo=FALSE}
ggplot() + 
  geom_histogram(aes(dataset$CreditLimit), bins = 40) +
  ggtitle('Avg Days of Pay vs Credit Limit') +
  xlab('Avg Days of Pay') + 
  ylab('Distribution')
```

# examine the relevance of all the dependent variables using a Decision Tree

```{r, echo=FALSE}

model <- rpart(CreditLimit ~ ., data = dataset_train)

```

```{r, echo=FALSE}

fancyRpartPlot(model)

```

```{r, echo=FALSE}

summary(model)

```

```{r, echo=FALSE, results='asis'}
#library(knitr)
#kable(model$variable.importance)

```

# what is the best cp to use?

```{r, echo=FALSE}

plotcp(model)

```

```{r, echo=FALSE}

model$cptable[which.min(model$cptable[,"xerror"]),"CP"]

```

# select the top relevant variables and revise our linear model

```{r, echo=FALSE}

model <- rpart(CreditLimit ~ DesignBandSalesLast12Mo + AvgDaysOfPayCategory + SalesLastYr + TotalSalesLast12Mo + AlternativeMetalSalesLast12Mo + MedianIncomeHouseholds + SalesCurrentYr + PlainBandSalesLast12Mo + MedianEarnings + RuralUrbanContinuumCode, data = dataset_train, control = rpart.control(cp = 0.01))

```

```{r, echo=FALSE}

fancyRpartPlot(model)

```

# Backward Elimination; remove the least relevant (highest p) variable and rebuild the model

```{r, echo=FALSE}

model <- rpart(CreditLimit ~ DesignBandSalesLast12Mo + AvgDaysOfPayCategory + SalesLastYr + TotalSalesLast12Mo + AlternativeMetalSalesLast12Mo + SalesCurrentYr + PlainBandSalesLast12Mo + MedianEarnings, data = dataset_train, control = rpart.control(cp = 0.01))

```

```{r, echo=FALSE}

fancyRpartPlot(model)

```

```{r, echo=FALSE}

summary(model)

```

```{r, echo=FALSE}

# aggregate(DesignBandSalesLast12Mo ~ CreditLimit, data = dataset_train, FUN = length)

```

# build a model based on sales related variables

```{r, echo=FALSE}
dataset_train_alt <- dataset_train

dataset_train_alt$CreditLimit <- factor(dataset_train_alt$CreditLimit) 

model <- rpart(CreditLimit ~ DesignBandSalesLast12Mo + SalesLastYr + TotalSalesLast12Mo + AlternativeMetalSalesLast12Mo + SalesCurrentYr + PlainBandSalesLast12Mo, data = dataset_train_alt, control = rpart.control(cp = 0.001), method="class")

```

```{r, echo=FALSE}

fancyRpartPlot(model)

```

```{r, echo=FALSE, include = FALSE}
# way to print the treeto a PDF document

# pdf("tree.pdf")
# fancyRpartPlot(model)
# dev.off()

```


```{r, echo=FALSE}

predictions <- predict(model, dataset_train_alt, type = "class")

```

# build a confusion matrix

```{r, echo=FALSE}

cm <- confusionMatrix(dataset_train_alt$CreditLimit, predictions)

ggplot(data = data.frame(cm$table), aes(x=Prediction, y=Reference)) +
  geom_tile(aes(fill=Freq)) + 
  scale_fill_gradient(low = "white", high = "steelblue") +
  theme(axis.text.x=element_text(angle=90,hjust=1)) +
  xlab("Predictions") +
  ylab("Ground Truth (Observations)") +
  geom_text(aes(label = Freq), size = 2) +
  ggtitle("Decison Tree Plot of Confusion Matrix")

```



```{r, echo=FALSE}

printcp(model)

```

# build a linear model using the same relevant variables which we have initially used in our decision tree

```{r, echo=FALSE}

model <- lm(formula = CreditLimit ~ DesignBandSalesLast12Mo + AvgDaysOfPay + SalesLastYr + TotalSalesLast12Mo + AlternativeMetalSalesLast12Mo + MedianIncomeHouseholds + SalesCurrentYr + PlainBandSalesLast12Mo + MedianEarnings + RuralUrbanContinuumCode, data = dataset_train)

```


```{r, echo=FALSE}

summary(model)

```

# examine a variation of a linear model using non-sales variables

```{r, echo=FALSE}

model <- lm(formula = CreditLimit ~ AvgDaysOfPay + MedianIncomeHouseholds + SalesCurrentYr + MedianEarnings, data = dataset_train)

```


```{r, echo=FALSE}

summary(model)

```

# ??? (we were examining the impact of standartizing )

```{r, echo=FALSE}

dataset_train_alt <- dataset_train

#dataset_train_alt$CreditLimit <- factor(dataset_train_alt$CreditLimit) 

model <- glm(CreditLimit ~ DesignBandSalesLast12Mo + SalesLastYr + AlternativeMetalSalesLast12Mo + MedianIncomeHouseholds + SalesCurrentYr + PlainBandSalesLast12Mo, family = gaussian, data=dataset_train_alt)

```

```{r, echo=FALSE}

summary(model)

```

# linear models using different "family" parameters

```{r, echo=FALSE}

dataset_train_alt <- dataset_train

#dataset_train_alt$CreditLimit <- factor(dataset_train_alt$CreditLimit) 

model <- glm(CreditLimit ~ 0 + DesignBandSalesLast12Mo + AlternativeMetalSalesLast12Mo + SalesCurrentYr + PlainBandSalesLast12Mo + DiamondBandSalesLast12Mo, family = gaussian, data=dataset_train_alt)


```


```{r, echo=FALSE}

summary(model)

```
```{r, echo=FALSE}

ggplot(data = dataset_train, aes(x = CreditLimit)) + geom_histogram(binwidth = 2000) + facet_wrap(~AvgDaysOfPayCategory)

```


```{r, echo=FALSE}

dataset_train_alt <- dataset_train

#dataset_train_alt$CreditLimit <- factor(dataset_train_alt$CreditLimit) 

model <- glm(CreditLimit ~ 0 + RuralUrbanContinuumCode + UrbanInfluenceCode + TotalPopulation + MedianIncomeHouseholds + NumberOfJewelryStores + NumberOfJewelryStoresState + JewelryStoreSalesState, family = gaussian, data=dataset_train_alt)


```


```{r, echo=FALSE}

summary(model)

```

```{r, echo=FALSE}

dataset_train_alt <- dataset_train

#dataset_train_alt$CreditLimit <- factor(dataset_train_alt$CreditLimit) 

model <- glm(CreditLimit ~ 0 + RuralUrbanContinuumCode + UrbanInfluenceCode + TotalPopulation + MedianIncomeHouseholds + NumberOfJewelryStores + NumberOfJewelryStoresState + JewelryStoreSalesState, family = poisson, data=dataset_train_alt)


```


```{r, echo=FALSE}

summary(model)

```

# how do these models compare to each other and to the first "naive" model?

```{r, echo=FALSE}

predictions <- predict(model, dataset_train_alt)

summary(predictions)
#helper.RMSE(dataset_train_alt$CreditLimit[is.na(predictions)==FALSE], predictions[is.na(predictions)==FALSE])

helper.RSquared(dataset_train_alt$CreditLimit[is.na(predictions)==FALSE], predictions[is.na(predictions)==FALSE])

```






