  ---
title: "First Model"
author:
  - name: Hakan Egeli
  - name: Soumyendu Sarkar
date: 'April 17, 2018'

output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project Setup

We chose to go with the ProjectTemplate package and structured our project accordingly. With this setup, our data gets automatically loaded from the data directory and using the munge feature we also automatically build a **training set** and a **test set** by dividing the original dataset using a 70/30 ratio. To make sure that both the training and the test sets have similar distributions, we used the `sample.split()` function and split the data over the **CreditLimit** variable.

We also set the seed to a fixed number so that we can produce the same sequence of random numbers between runs and our results are not effected by the changes in the datasets.

```{r, echo=FALSE, include = FALSE}

# load the project. This will autoload the data and also create training and test datasets
library('ProjectTemplate')
load.project()

```

## Pre-Model Work

We developed our own functions to be able to calculate the errors for our model and our future models. These error functions include **R-squared**, **Adjusted R-squared** and the **Root Mean Square Error (RMSE)**. We will make use of these functions to track the fitness and the performance of our models, and we will be able to compare models to each other.

```{r, echo=FALSE, include = FALSE}

# define some of the error estimaste functions here to be used later

RSquared <- function(observed, predicted){

  ss_res <- sum((observed - predicted)^2)
  ss_tot <- sum((observed - mean(observed))^2)
  
  1 - (ss_res/ss_tot)
}

RSquaredAdjsuted <- function(R2, n, p){
  1 - (1 - R2)*((n - 1)/(n - p - 1))
}

RMSE <- function(observed, predicted){
  sqrt(mean((observed - predicted)^2))
}
```

```{r, echo=FALSE, include = FALSE}
#
# This function allows multiple plots to be displayed in columns and rows 
#
# Credit Cookbook for R (http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/)
#

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

## Initial Model

We wanted to see if Sales would be a good indicator to predict Credit Limits. We chose the Last Year&apos;s Sales (SalesLastYr) column and build a linear regression model with one independent variable. Our goal is to see whether we can formulate the Credit Limit with a simple equation as the following and whether this realtionship is good enough to get acceptable predictions for Credit Limits:

$$Credit Limit = w*Previous Years Sales+b$$ 

```{r}
model <- lm(formula = CreditLimit ~ SalesLastYr, data = dataset_train)
```


```{r, echo=FALSE}
modelSummary <-summary(model)

print(modelSummary)
```

The `summary()` results of this model trained using the **training set** shows that both the intercept (b) and the coefficient (w) are significant (P values are much less than 0.05). This might be a good start but how good of a fit is this model to our data?

R-squared paramater indicates the the fitness of a model and we want this value to be closer to 1, aka we want the ratio of the sum of the square of the residuals ($SS_r$) over the sum of the square of the averages ($SS_t$) to be small so that $R^2$ becomes larger.

$$R^2 = 1 - SS_r/SS_t$$ 

The value 0.03102 for the $R^2$ seemed low which is indicating that the model is not a good fit. And at this time we are not concerned with the Adjusted $R^2$ since we have only one independent variable in our model.

The three of the indicators that we are planning on using for this model and for all of our future models are; $R^2$, Adjusted $R^2$ and the RMSE and below is a recap of these three indicators for our **trained model**:

```{r, echo=FALSE, include = FALSE}
y <- dataset_train$CreditLimit
y_hat <- predict(model, dataset_train)

```

```{r, echo=FALSE}
# calculate R-squared
R2 <- RSquared(y, y_hat)
cat('R Squared: ', R2, "\n")

# calculate Adjusted R-squared
R2Adj <- RSquaredAdjsuted(R2, length(y), 1)
cat('R Squared (Adjusted): ', R2Adj, "\n")

# calculate Root Mean Squared error
rmse <- RMSE(y, y_hat)
cat('Root Mean Square Error: ', rmse, "\n")

```

```{r, echo=FALSE, include = FALSE}

cat('Abs diff between summary$r.squared and RSquared() is less than 1e-10:', abs(modelSummary$r.squared - R2) < 1e-10 , '\n' )

cat('Abs diff between summary$adj.r.squared and RSquaredAdjsuted() is less than 1e-10:', abs(modelSummary$adj.r.squared - R2Adj) < 1e-10 , '\n' )

```

And below are the results of the same three indicators for our model using the **test set**:

```{r, echo=FALSE, include = FALSE}
y <- dataset_test$CreditLimit
y_hat <- predict(model, dataset_test)

```

```{r, echo=FALSE}
# calculate R-squared
R2 <- RSquared(y, y_hat)
cat('R Squared: ', R2, "\n")

# calculate Adjusted R-squared
R2Adj <- RSquaredAdjsuted(R2, length(y), 1)
cat('R Squared (Adjusted): ', R2Adj, "\n")

# calculate Root Mean Squared error
rmse <- RMSE(y, y_hat)
cat('Root Mean Square Error: ', rmse, "\n")

```

The $R^2$ value is slightly better for the test set than the training set but in both cases these values still indicate not a good fit. The RSME values for both the raining and the test sets seems to be close to each other and we hope to see this number go lower as we build better models.

## Data Insights

We have performed other exploratory analysis on our data and we began our examination with the **Sales** numbers for the last calender year. We looked at the min, max, std dev and the quintiles to understand the spread of the data as well as to be able to do a comparison between the training and the test sets.

#### Training Set Summary

```{r, echo=FALSE}
summary(dataset_train$SalesLastYr)
```

```{r, echo=FALSE}
cat('Std Dev.', sd(dataset_train$SalesLastYr))
```

#### Test Set Summary

```{r, echo=FALSE}
summary(dataset_test$SalesLastYr)
```
```{r, echo=FALSE}
cat('Std Dev.', sd(dataset_test$SalesLastYr))
```

Both in the training set and the test set these numbers seemed to be close to each other and we do notice that the Sales numbers are clustred more towards the left of the scale. To visulaize this better, we have created a histogram of the sales numbers for the traning set and the test set so that we can do a side by side analysis.

#### Comparison of Distribution of Sales Between the Training Set and the Test Set

```{r, echo=FALSE}
hist_train <- ggplot() +
  geom_histogram(aes(dataset_train$SalesLastYr), bins = 40) +
  ggtitle('Last Year\'s Sales - Training Set') +
  xlab('Sales') +
  ylab('Count')

hist_test <- ggplot() +
  geom_histogram(aes(dataset_test$SalesLastYr), bins = 40) +
  ggtitle('Last Year\'s Sales - Test Set') +
  xlab('Sales') +
  ylab('Count')
```


```{r, echo=FALSE}
multiplot(hist_train, hist_test, cols=2)
```

Upon visual inspection of the **Sales** data between the training and the test sets, we do see that they are distributed similarly.

We also wanted to see how the distrubution of Last Year&apos;s Sales looked like compared to the distribution of Credit Limits for the same data points.

```{r, echo=FALSE}
ggplot() +
  geom_density(aes(dataset_train$SalesLastYr), color = 'red') +
  ggtitle('Distribution of Sales - Training Set') +
  xlab('Last Year\'s Sales') +
  ylab('Density')
```


```{r, echo=FALSE}
ggplot() +
  geom_density(aes(dataset_train$CreditLimit), color = 'red') +
  ggtitle('Distribution of Credit Limit - Training Set') +
  xlab('Credit Limit') +
  ylab('Density')
```

The Sales data looks like a normal distribution and we were hoping to see the same for the plot of the Credit Limits for our tranining data.

## Performance of the Model

To better understand the correlation between these two variables and how our model repsresents this data, we have plotted the **Credit Limit vs. Last Year&apos;s Sales** and also plotted a line representing our model.

```{r, echo=FALSE}
ggplot() + 
  geom_point(aes(x = dataset_train$SalesLastYr, y = dataset_train$CreditLimit), color = 'green') +
  geom_point(aes(x = dataset_test$SalesLastYr, y = dataset_test$CreditLimit), color = 'blue') +
  geom_line(aes(x = dataset$SalesLastYr, y = predict(model, newdata = dataset)), color = 'red') +
  ggtitle('Last Year Sales vs Credit Limit Compared to Training and Test Sets') +
  xlab('Last Year\'s Sales') + 
  ylab('Credit Limit')
```

From the plot of the data Credit Limit vs the Last Year&apos;s Sales, we do see that the two variables are not correlated and that is why our linear model is not performing well.

It is possible that the Credit Limit cannot be explained by one varaible and a linear relationship might still exist using other independent variables. It is also possible that the relationships are non-linear and we might have to try non-linear models to explain the data.

## Next Step

We will try to improve our linear model by examining all other variables that we have in our dataset and use a **Backward Elimitaion** to find out the most significant variables which might help us to build a better model. As we add more variables to describe our model we will also utilize the Adjusted $R^2$ predictor to account for the impact of the added variables to the $R^2$ value.

Future steps might also include adding non-linear terms to the model or building other non-linear models such as one which uses a **Decision Tree** or take advantage of ensemble learning by building a model using **Random Forest**.

We will evaluate the performance of these models by comparing the RMSE values to see which model will be able to reduce the error the most.


